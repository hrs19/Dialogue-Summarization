{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6c2181-84c4-4240-b160-680ae8909f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /shared/centos7/cuda/12.1/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/sampgaon.h/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sampgaon.h/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset,load_metric\n",
    "# from evaluate import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# device = 'cuda:0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import emoji\n",
    "emoji_list = emoji.EMOJI_DATA.keys()\n",
    "emoji_descriptions = [emoji.demojize(e, delimiters=(\"<\", \">\")) for e in emoji_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93d5e4-656a-4f24-b217-cd4a0b0f3738",
   "metadata": {},
   "source": [
    "### Loading the base models from Huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5114a916-34cc-4803-8e31-143a53b76f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(35888, 512)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_16_bit = False\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "if load_16_bit:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.float16).to(device)\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769daa24-98a1-4370-b59d-1096b6b0d26e",
   "metadata": {},
   "source": [
    "### Updating Tokenizer and model embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99194f6-e9ed-4e8e-b2d0-abf766065d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = ['<file_photo>','<file_picture>','<file_other>','<file_video>','<file_image>','<file_gif>']\n",
    "new_tokens = new_tokens+emoji_descriptions\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': new_tokens})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae3607-9604-40a2-9eb2-d50d115715e1",
   "metadata": {},
   "source": [
    "### Loading SAMSum Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd58cf1-50f2-4805-b573-919e6b7a25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Samsum dataset\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"] \n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ebc6c-3614-49dc-897f-01f9332f372f",
   "metadata": {},
   "source": [
    "### Tokenizing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "166dc74d-ca09-4c69-bbc8-c7a2b55e20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenzing\n",
    "def tokenize_function(tokenizer,examples):\n",
    "    input_dialogues = [\"Summarize dialogue >>\\n \" + emoji.demojize(dialogue, delimiters=(\"<\", \">\")) for dialogue in examples[\"dialogue\"]]\n",
    "    inputs = tokenizer(input_dialogues, padding=\"max_length\", truncation=True, max_length=1000)\n",
    "    targets = tokenizer(examples[\"summary\"], padding=\"max_length\", truncation=True, max_length=100)\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"labels\": targets[\"input_ids\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4498f9b6-92d6-414f-95c5-15762f8c1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokenized = train_data.map(lambda examples: tokenize_function(tokenizer, examples), batched=True)\n",
    "validation_data_tokenized = validation_data.map(lambda examples: tokenize_function(tokenizer, examples), batched=True)\n",
    "test_data_tokenized = test_data.map(lambda examples: tokenize_function(tokenizer, examples), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c954d3-8bc5-46d2-93a3-416736d3cefa",
   "metadata": {},
   "source": [
    "### Sample summarization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70208a93-11e8-4dbc-869f-d8c538fa5d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize dialogue >>\n",
      " Rachel: <file_other>\n",
      "Rachel: Top 50 Best Films of 2018\n",
      "Rachel: :)\n",
      "Janice: Omg, I've watched almost all 50... xDD\n",
      "Spencer: Hahah, Deadpool 2 also??\n",
      "Janice: Yep\n",
      "Spencer: Really??\n",
      "Janice: My bf forced me to watch it xD\n",
      "Rachel: Hahah\n",
      "Janice: It wasn't that bad\n",
      "Janice: I thought it'd be worse\n",
      "Rachel: And Avengers? :D\n",
      "Janice: 2 times\n",
      "Rachel: Omg\n",
      "Janice: xP\n",
      "Rachel: You are the best gf in the world\n",
      "Rachel: Your bf should appreciate that ;-)\n",
      "Janice: He does\n",
      "Janice: x)\n",
      "summary: Janice has watched almost all films of 2018.\n"
     ]
    }
   ],
   "source": [
    "def summarize(tokenizer,model,text):\n",
    "    print(f\"Summarize dialogue >>\\n {emoji.demojize(text, delimiters=('<', '>'))}\")\n",
    "    inputs = tokenizer(f\"Summarize dialogue >>\\n {emoji.demojize(text, delimiters=('<', '>'))}\", return_tensors=\"pt\", max_length=1000, truncation=True, padding=\"max_length\").to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs.input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
    "    # Decode the summary\n",
    "    summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    return summary[0]\n",
    "\n",
    "text = test_data['dialogue'][-1]\n",
    "print(\"summary:\",summarize(tokenizer,model,text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1d5d1-d156-4685-bf50-69e275979b3f",
   "metadata": {},
   "source": [
    "### Model - FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d08b7e7-c462-481d-858b-092985787a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 80811392\n"
     ]
    }
   ],
   "source": [
    "# for name, param in bart_base_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable parameters:\", trainable_params)        \n",
    "#247577856/142329600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636dea2-0eec-418b-a55c-19e7db3ae36a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61748186-d405-4bdc-a4e6-05da0b527fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sampgaon.h/.local/lib/python3.8/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./flan_t5_small_full_finetune\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "#     eval_device='cpu',\n",
    "    overwrite_output_dir = False,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=40,\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data_tokenized,\n",
    "    eval_dataset=validation_data_tokenized,\n",
    "#     compute_metrics=lambda pred: rouge_metric.compute(predictions=pred.predictions, references=pred.label_ids)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7057337-2840-4833-8c9a-96b3c65139e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sampgaon.h/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147320' max='147320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147320/147320 2:56:59, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.310300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>0.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>0.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>0.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>0.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>0.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>0.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>0.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>0.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>0.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>0.296100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>0.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>0.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>0.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>0.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>0.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>0.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>0.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>0.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>0.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>0.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>0.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>0.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>0.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>0.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>0.277200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>0.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>0.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>0.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>0.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>0.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>0.283900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>0.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>0.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>0.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>0.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>0.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>0.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146500</td>\n",
       "      <td>0.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>0.283300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=147320, training_loss=0.0716666152125754, metrics={'train_runtime': 10619.9787, 'train_samples_per_second': 55.488, 'train_steps_per_second': 13.872, 'total_flos': 2.2075496103936e+17, 'train_loss': 0.0716666152125754, 'epoch': 40.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c49bf13-5da4-47e9-ab84-0396a24d4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained('./flan_t5_small_full_finetune')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "586e004e-35ee-472b-ad92-30c4edaa9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'eval_runtime': 0.0016, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./flan_t5_small_full_finetune_save')\n",
    "tokenizer.save_pretrained('./tokenizer-emoji')\n",
    "\n",
    "model_tok_save_directory = \"./flan_t5_small_full_finetune_model_tok\"\n",
    "model.save_pretrained(model_tok_save_directory)\n",
    "tokenizer.save_pretrained(model_tok_save_directory)\n",
    "\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=validation_data_tokenized,\n",
    "    compute_metrics=lambda pred: rouge_metric.compute(predictions=pred.predictions, references=pred.label_ids)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = eval_trainer.evaluate(test_data)\n",
    "# results = trainer.evaluate(test_data)\n",
    "print(\"ROUGE scores:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef34249-b250-4e71-a750-9bf153494f9b",
   "metadata": {},
   "source": [
    "### ROUGE SCORE -- inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e1b6e7-7cc1-4622-8293-3db53f3e34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(tokenizer,model,text):\n",
    "    \"\"\"\n",
    "    Summarizes the given text using the provided tokenizer and model.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): The tokenizer used to tokenize the input text.\n",
    "        model (Model): The model used for summarization.\n",
    "        text (str): The text to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the summarized text.    \n",
    "    \"\"\"        \n",
    "#     print(f\"Summarize dialogue >>\\n {emoji.demojize(text, delimiters=('<', '>'))}\")\n",
    "    inputs = tokenizer(f\"Summarize dialogue >>\\n {emoji.demojize(text, delimiters=('<', '>'))}\", return_tensors=\"pt\", max_length=1000, truncation=True, padding=\"max_length\").to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs.input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
    "    # Decode the summary\n",
    "    summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "generated_summaries = []\n",
    "actual_summaries = []\n",
    "generated_summary_orignal = []\n",
    "dialogue_list = []\n",
    "SAVED_MODEL_PATH = './flan_t5_small_full_finetune_save-30'\n",
    "SAVED_TOK_PATH = \"./flan_t5_small_full_finetune_model_tok-30\"\n",
    "SAVED_MODEL_TOK = AutoTokenizer.from_pretrained(SAVED_TOK_PATH)#.to(device)\n",
    "SAVED_MODEL = AutoModelForSeq2SeqLM.from_pretrained(SAVED_MODEL_PATH).to(device)\n",
    "\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "orignal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "orignal_model_tok = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d5e27-0090-4809-827f-a748369a3e80",
   "metadata": {},
   "source": [
    "### Inference loop to generate summaries on test dataset for ROUGE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8314cf5e-5565-4f89-a81e-fa0628a5c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples summarized:10\ttime:8.748680353164673\n",
      "samples summarized:20\ttime:18.305094480514526\n",
      "samples summarized:30\ttime:28.15091037750244\n",
      "samples summarized:40\ttime:36.07048511505127\n",
      "samples summarized:50\ttime:45.00730299949646\n",
      "samples summarized:60\ttime:53.23375463485718\n",
      "samples summarized:70\ttime:61.01580595970154\n",
      "samples summarized:80\ttime:70.02505016326904\n",
      "samples summarized:90\ttime:78.31338310241699\n",
      "samples summarized:100\ttime:87.04405760765076\n",
      "samples summarized:110\ttime:93.28603911399841\n",
      "samples summarized:120\ttime:103.0273072719574\n",
      "samples summarized:130\ttime:111.52047419548035\n",
      "samples summarized:140\ttime:121.4963366985321\n",
      "samples summarized:150\ttime:129.78151988983154\n",
      "samples summarized:160\ttime:139.65385794639587\n",
      "samples summarized:170\ttime:148.27914929389954\n",
      "samples summarized:180\ttime:156.5907347202301\n",
      "samples summarized:190\ttime:166.19008612632751\n",
      "samples summarized:200\ttime:175.50325345993042\n",
      "samples summarized:210\ttime:185.2167510986328\n",
      "samples summarized:220\ttime:194.69765305519104\n",
      "samples summarized:230\ttime:204.18043899536133\n",
      "samples summarized:240\ttime:212.45257306098938\n",
      "samples summarized:250\ttime:222.0153272151947\n",
      "samples summarized:260\ttime:230.24914717674255\n",
      "samples summarized:270\ttime:239.73247575759888\n",
      "samples summarized:280\ttime:247.10259175300598\n",
      "samples summarized:290\ttime:256.4302237033844\n",
      "samples summarized:300\ttime:267.0003294944763\n",
      "samples summarized:310\ttime:278.300345659256\n",
      "samples summarized:320\ttime:287.500040769577\n",
      "samples summarized:330\ttime:294.3663740158081\n",
      "samples summarized:340\ttime:304.2222559452057\n",
      "samples summarized:350\ttime:311.31978583335876\n",
      "samples summarized:360\ttime:320.5233311653137\n",
      "samples summarized:370\ttime:329.32892179489136\n",
      "samples summarized:380\ttime:339.98125100135803\n",
      "samples summarized:390\ttime:348.08454155921936\n",
      "samples summarized:400\ttime:358.1422414779663\n",
      "samples summarized:410\ttime:365.8876266479492\n",
      "samples summarized:420\ttime:374.3954360485077\n",
      "samples summarized:430\ttime:384.96184968948364\n",
      "samples summarized:440\ttime:395.7523500919342\n",
      "samples summarized:450\ttime:405.89159202575684\n",
      "samples summarized:460\ttime:415.4550142288208\n",
      "samples summarized:470\ttime:423.6108593940735\n",
      "samples summarized:480\ttime:433.25200748443604\n",
      "samples summarized:490\ttime:441.0524673461914\n",
      "samples summarized:500\ttime:451.04867720603943\n",
      "samples summarized:510\ttime:460.30915570259094\n",
      "samples summarized:520\ttime:467.4774389266968\n",
      "samples summarized:530\ttime:475.76838850975037\n",
      "samples summarized:540\ttime:484.68077397346497\n",
      "samples summarized:550\ttime:491.9359862804413\n",
      "samples summarized:560\ttime:501.62002968788147\n",
      "samples summarized:570\ttime:508.31889963150024\n",
      "samples summarized:580\ttime:515.8977499008179\n",
      "samples summarized:590\ttime:524.4456853866577\n",
      "samples summarized:600\ttime:533.9011769294739\n",
      "samples summarized:610\ttime:542.8001389503479\n",
      "samples summarized:620\ttime:552.2869527339935\n",
      "samples summarized:630\ttime:562.9828824996948\n",
      "samples summarized:640\ttime:572.2666578292847\n",
      "samples summarized:650\ttime:581.5371263027191\n",
      "samples summarized:660\ttime:590.2976207733154\n",
      "samples summarized:670\ttime:599.727881193161\n",
      "samples summarized:680\ttime:609.2336618900299\n",
      "samples summarized:690\ttime:616.9728448390961\n",
      "samples summarized:700\ttime:627.4086401462555\n",
      "samples summarized:710\ttime:637.1907916069031\n",
      "samples summarized:720\ttime:645.3466458320618\n",
      "samples summarized:730\ttime:654.1107769012451\n",
      "samples summarized:740\ttime:662.474054813385\n",
      "samples summarized:750\ttime:673.3188235759735\n",
      "samples summarized:760\ttime:682.921014547348\n",
      "samples summarized:770\ttime:691.9975695610046\n",
      "samples summarized:780\ttime:699.6853544712067\n",
      "samples summarized:790\ttime:708.7962548732758\n",
      "samples summarized:800\ttime:717.5889892578125\n",
      "samples summarized:810\ttime:724.456780910492\n",
      "Total time taken: 734.3901863098145 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "i=1\n",
    "j=0\n",
    "for example in test_data:\n",
    "    if i%10==0:\n",
    "#         print()\n",
    "        j+=10\n",
    "        print(f\"samples summarized:{j}\\ttime:{time.time()-start_time}\")\n",
    "#         print(f\"\",)\n",
    "#     print(example['dialogue'])\n",
    "#     generated_summary = summarize(SAVED_MODEL_TOK,SAVED_MODEL,example['dialogue'])\n",
    "#     combined_model\n",
    "    generated_summary = summarize(SAVED_MODEL_TOK,SAVED_MODEL,example['dialogue'])\n",
    "    \n",
    "    generated_summaries.append(generated_summary[0])\n",
    "    generated_summary_o = summarize(orignal_model_tok,orignal_model,example['dialogue'])\n",
    "    generated_summary_orignal.append(generated_summary_o[0])\n",
    "    actual_summaries.append(example[\"summary\"])\n",
    "    dialogue_list.append(example['dialogue'])\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b8b4d-5557-440a-a071-eac9d527ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame({'finetune_summary':generated_summaries,'original_summary':generated_summary_orignal,'human_summary':actual_summaries,'dialog':dialogue_list})\n",
    "temp_df.to_csv('full_flan_t5_small_results_30_epoch.csv')#.loc[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2517522-6fbc-4bf0-a3c8-ea990812f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('full_flan_t5_small_results_20_epoch.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591ebe0-a11d-441c-b5ea-c94305304b97",
   "metadata": {},
   "source": [
    "### ROUGE for fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5309c1ae-1332-44b0-ac8f-51cf875173d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.6044666767120361 seconds\n",
      "Total time taken: 0.0003504753112792969 seconds\n",
      "Average ROUGE scores:\n",
      "rouge1: 0.4512592182549751\n",
      "rouge2: 0.2085953067706064\n",
      "rougeL: 0.37440036871614574\n"
     ]
    }
   ],
   "source": [
    "generated_summary_orignal = df['original_summary']\n",
    "actual_summaries = df['human_summary']\n",
    "generated_summaries = df['finetune_summary']\n",
    "from rouge_score import rouge_scorer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = [scorer.score(actual_summary, gen_summary) for actual_summary, gen_summary in zip(actual_summaries, generated_summary_orignal)]\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# To calculate average scores\n",
    "average_scores = {}\n",
    "for key in scores[0].keys():\n",
    "    average_scores[key] = sum(score[key].fmeasure for score in scores) / len(scores)\n",
    "\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")    \n",
    "print(\"Average ROUGE scores:\")\n",
    "for key, value in average_scores.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c0ab2-5917-45d4-b4c4-27cc5fa6bbdd",
   "metadata": {},
   "source": [
    "### ROUGE for fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f120ee-dfcb-47c4-abff-387b5e1bdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary_orignal = df['original_summary']\n",
    "actual_summaries = df['human_summary']\n",
    "generated_summaries = df['finetune_summary']\n",
    "from rouge_score import rouge_scorer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = [scorer.score(actual_summary, gen_summary) for actual_summary, gen_summary in zip(actual_summaries, generated_summaries)]\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# To calculate average scores\n",
    "average_scores = {}\n",
    "for key in scores[0].keys():\n",
    "    average_scores[key] = sum(score[key].fmeasure for score in scores) / len(scores)\n",
    "\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")    \n",
    "print(\"Average ROUGE scores:\")\n",
    "for key, value in average_scores.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
